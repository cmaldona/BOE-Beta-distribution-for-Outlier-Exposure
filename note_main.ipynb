{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2587b0ea-8808-4829-87f5-b7f2f535fff5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567d5f6-5c7f-47de-a448-b1b83e1c1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    RobertaModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    ")\n",
    "\n",
    "from functions import *\n",
    "from data_scripts import get_setups\n",
    "\n",
    "\n",
    "log_file_path = os.path.join('', 'execution_time_log.txt')\n",
    "log_file = open(log_file_path, 'a')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gpu_id = 0\n",
    "total_memory = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "used_memory = torch.cuda.memory_allocated(gpu_id)\n",
    "reserved_memory = torch.cuda.memory_reserved(gpu_id)\n",
    "\n",
    "# Impresión de información sobre la memoria de la GPU\n",
    "print(f\"Total memory: {total_memory / 1e9} GB\")\n",
    "print(f\"Used memory: {used_memory / 1e9} GB\")\n",
    "print(f\"Reserved memory: {reserved_memory / 1e9} GB\")\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "# Registrar el tiempo de carga de datos\n",
    "data_load_start_time = time.time()\n",
    "experiments = get_setups()\n",
    "data_load_end_time = time.time()\n",
    "log_file.write(f\"Data loading time: {data_load_end_time - data_load_start_time} seconds\\n\")\n",
    "\n",
    "params_grid = ParameterGrid({\n",
    "    'setup': list(experiments.items()),\n",
    "    'structure': ['peft'],\n",
    "    'seed': [97,199, 103, 23, 137],\n",
    "    'regularizer': [1, 0],\n",
    "    'divergence_metric': ['kl', 'tv'],\n",
    "    'distribution': ['Uniform', 'Beta'],\n",
    "})\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "base_path = 'baseline'\n",
    "excel_file = base_path+'/eval.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8379f-adc2-43d2-b0fd-5a114cafec26",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539f901-2537-487d-b15c-1fb4fdef4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución del experimento para cada configuración de parámetros\n",
    "for params in params_grid:\n",
    "\n",
    "    print('params:\\n')\n",
    "    pprint.pprint(params)\n",
    "\n",
    "    all_results = []\n",
    "    setup = params['setup'][1]\n",
    "    N = 0\n",
    "    sheet_name = f'seed {params[\"seed\"]}'\n",
    "\n",
    "    # Obtener k_classes del dataset de entrenamiento\n",
    "    generalisation = []\n",
    "    train_dataloader = setup['train_dataloader']\n",
    "    labels = []\n",
    "    for batch in train_dataloader:\n",
    "        N += batch['input_ids'].shape[0]\n",
    "        generalisation.extend(batch['generalisation'].numpy())\n",
    "        labels.extend(batch['labels'].numpy())\n",
    "\n",
    "    mask_id = np.array(generalisation) == 1\n",
    "    k_classes = len(set(np.array(labels)[mask_id]))\n",
    "\n",
    "    # Configuración del dispositivo y la pérdida personalizada\n",
    "    device = prepare_environment(seed=params['seed'], device=\"cuda\")\n",
    "    custom_loss_function = CustomLoss(\n",
    "        loss_type=params['divergence_metric'],\n",
    "        weight_regularizer=params['regularizer'],\n",
    "        num_classes=k_classes,\n",
    "        distribution=params['distribution']\n",
    "    )\n",
    "\n",
    "    model, optimizer = initialize_model(params['structure'], k_classes, peft_config, device)\n",
    "\n",
    "    # Entrenamiento y evaluación del modelo\n",
    "    flag_peft = True if params['structure'] == 'peft' else False\n",
    "    start_time_run = time.time()\n",
    "    val_metrics, train_metrics = train_evaluate(\n",
    "        model,\n",
    "        setup['train_dataloader'],\n",
    "        setup['eval_dataloader'],\n",
    "        optimizer,\n",
    "        EPOCHS,\n",
    "        flag_peft,\n",
    "        custom_loss_function\n",
    "    )\n",
    "    end_time_run = time.time()\n",
    "    log_file.write(\n",
    "        f\"Training and validation time for Normal, params:\\n {params}.\\n {start_time_run - end_time_run} seconds\\n\"+\"--\"*20\n",
    "    )\n",
    "    print(f\"Training and validation time for Normal, params:\\n {params}.\\n {start_time_run - end_time_run} seconds\\n\"+\"--\"*20)\n",
    "\n",
    "    total_time = end_time_run - start_time_run\n",
    "    minutes = int(total_time // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    print(f'Total time: {minutes}:{seconds}')\n",
    "    print(train_metrics)\n",
    "    print(val_metrics)\n",
    "\n",
    "    model_path_saved = save_results(model, val_metrics, train_metrics, params, base_path, k_classes)\n",
    "    params_str = ', '.join([f'({key}:{value})' for key, value in params.items()])\n",
    "\n",
    "    # Guardar resultados adicionales según el valor de regularizer\n",
    "    if params['regularizer'] == 0:\n",
    "        # MSP\n",
    "        metrics = eval_model(model, setup['eval_dataloader'])\n",
    "        metrics['tech'] = 'MSP'\n",
    "        metrics['params'] = params_str\n",
    "        all_results.append(metrics)\n",
    "\n",
    "        # ViM\n",
    "        for D in [k_classes, 256, 512]:\n",
    "            u, NS, alpha = get_vim_model(model, setup['train_dataloader'], dim=D)\n",
    "            metrics = test_model_vim(model, setup['eval_dataloader'], alpha, u, NS)\n",
    "            metrics['tech'] = f'ViM(dim={D})'\n",
    "            metrics['params'] = params_str\n",
    "            all_results.append(metrics)\n",
    "    else:\n",
    "        # OE + Uniform\n",
    "        metrics = eval_model(model, setup['eval_dataloader'])\n",
    "        metrics['tech'] = 'OE+'+params['distribution']\n",
    "        metrics['params'] = params_str\n",
    "        all_results.append(metrics)\n",
    "\n",
    "    # Guardar resultados en Excel\n",
    "    if not os.path.exists(excel_file):\n",
    "        new_df = pd.DataFrame(columns=list(metrics.keys()))\n",
    "        for dic_data in all_results:\n",
    "            new_df.loc[len(new_df)] = list(dic_data.values())\n",
    "        new_df.to_excel(excel_file, sheet_name=sheet_name)\n",
    "    else:\n",
    "        with pd.ExcelWriter(excel_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            if sheet_name in writer.sheets:\n",
    "                df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "            else:\n",
    "                df = pd.DataFrame(columns=list(metrics.keys()))\n",
    "            # Asegurarse de que las columnas de df coincidan con las claves del diccionario\n",
    "            for dic_data in all_results:\n",
    "                if not set(dic_data.keys()).issubset(df.columns):\n",
    "                    raise ValueError(\n",
    "                        f\"Las claves del diccionario no coinciden con las columnas del DataFrame.\\nClaves del diccionario: {dic_data.keys()}\\nColumnas del DataFrame: {df.columns}\")\n",
    "\n",
    "                # Asegúrate de que las columnas estén en el orden correcto\n",
    "                df = df.reindex(columns=dic_data.keys())\n",
    "\n",
    "                # Añadir la fila de datos al DataFrame\n",
    "                df.loc[len(df)] = list(dic_data.values())\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c411a-cf01-4a40-b1d6-13e651932af2",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58030101-dd7e-4ff4-a5d9-e785a7a34a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configuración de PEFT\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Ruta al archivo de prueba\n",
    "test_excel_file = base_path + '/test_propose_v1.xlsx'\n",
    "\n",
    "# Fase de Testing Adicional\n",
    "for name_setup, dataloaders in list(experiments.items())[3:]:\n",
    "    train_dataloader = dataloaders['train_dataloader']\n",
    "    evaluation_dataloader = dataloaders['test_dataloader']\n",
    "\n",
    "    path_to_models = f'{base_path}/models/'\n",
    "    model_files = os.listdir(path_to_models)\n",
    "    all_results = []\n",
    "    sheet_name = name_setup\n",
    "\n",
    "    # Filtrar los archivos de modelos que contienen \"20NG\" en su nombre\n",
    "    filtered_models = [model for model in model_files if (\"NG20\" in model) and (\"distribution:Beta\" in model)]\n",
    "\n",
    "    for selected_model_path in filtered_models:\n",
    "        print(selected_model_path)\n",
    "        if name_setup not in selected_model_path:\n",
    "            continue\n",
    "\n",
    "        selected_model_path = os.path.join(path_to_models, selected_model_path)\n",
    "        print(f\"Procesando el modelo: {selected_model_path}\")\n",
    "\n",
    "        match_seed = re.search(r'seed:(\\d+)', selected_model_path)\n",
    "        k_classes = re.search(r'k: (\\d+)', selected_model_path)\n",
    "        distribution_match = re.search(r'distribution:([a-zA-Z]+)', selected_model_path)\n",
    "        distribution_name = distribution_match.group(1)\n",
    "        seed_model = int(match_seed.group(1))\n",
    "\n",
    "        params_str = re.findall(r'\\((.*?)\\).pth', selected_model_path)[0]\n",
    "        params_dict = {}\n",
    "\n",
    "        pattern = r'(\\w+):(\\([^)]+\\)(?:,\\s*\\([^)]+\\))*|[^\\,]+)'\n",
    "\n",
    "        matches = re.finditer(pattern, params_str)\n",
    "        for match in matches:\n",
    "            key = match.group(1).strip()\n",
    "            value = match.group(2).strip()\n",
    "            try:\n",
    "                value = eval(value)\n",
    "            except:\n",
    "                pass\n",
    "            params_dict[key] = value\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            'roberta-base',\n",
    "            num_labels=int(k_classes.group(1)),\n",
    "            return_dict=True\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.load_state_dict(torch.load(selected_model_path))\n",
    "        model.to(device)\n",
    "\n",
    "        if 'regularizer:0' in selected_model_path:\n",
    "            # MSP\n",
    "            metrics = eval_model(model, evaluation_dataloader)\n",
    "            metrics['tech'] = 'MSP'\n",
    "            metrics.update(params_dict)\n",
    "            all_results.append(metrics)\n",
    "\n",
    "            # ViM\n",
    "            D = 256\n",
    "            u, NS, alpha = get_vim_model(model, train_dataloader, dim=D)\n",
    "            metrics = test_model_vim(model, evaluation_dataloader, alpha, u, NS)\n",
    "            metrics['tech'] = f'ViM(dim={D})'\n",
    "            metrics.update(params_dict)\n",
    "            all_results.append(metrics)\n",
    "        else:\n",
    "            # OE + Uniform\n",
    "            metrics = eval_model(model, evaluation_dataloader)\n",
    "            metrics['tech'] = 'OE+Beta'\n",
    "            metrics.update(params_dict)\n",
    "            all_results.append(metrics)\n",
    "\n",
    "    if len(all_results) == 0:\n",
    "        continue\n",
    "\n",
    "    # Guardar resultados de Testing en Excel\n",
    "    if not os.path.exists(test_excel_file):\n",
    "        new_df = pd.DataFrame(columns=list(metrics.keys()))\n",
    "        for dic_data in all_results:\n",
    "            new_df.loc[len(new_df)] = list(dic_data.values())\n",
    "        new_df.to_excel(test_excel_file, sheet_name=sheet_name, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(test_excel_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "            # Verificar si la hoja ya existe\n",
    "            if sheet_name in writer.sheets:\n",
    "                df = pd.read_excel(test_excel_file, sheet_name=sheet_name)\n",
    "            else:\n",
    "                df = pd.DataFrame(columns=list(metrics.keys()))\n",
    "    \n",
    "            # Eliminar la columna 'Unnamed: 0' si existe\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "            for dic_data in all_results:\n",
    "                # Verificar si las claves del diccionario coinciden con las columnas del DataFrame\n",
    "                missing_cols = set(dic_data.keys()) - set(df.columns)\n",
    "                if missing_cols:\n",
    "                    # Agregar columnas faltantes\n",
    "                    for col in missing_cols:\n",
    "                        df[col] = None  # Rellenar con valores por defecto (None o np.nan)\n",
    "    \n",
    "                # Asegúrate de que las columnas estén en el orden correcto\n",
    "                ordered_values = [dic_data.get(col, None) for col in df.columns]\n",
    "    \n",
    "                # Añadir la fila de datos al DataFrame\n",
    "                df.loc[len(df)] = ordered_values\n",
    "    \n",
    "            # Guardar el DataFrame actualizado en la hoja correspondiente\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ae757-1da5-4438-903b-c6ce62975898",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = pd.read_excel(base_path + '/test_propose_v1.xlsx')\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4026d95-b7d4-420f-a363-8851cfc787a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a considerar\n",
    "columns_to_consider = [\n",
    "    'acc', 'f1_m', 'f1_M', 'AUROC_ID/far', 'FPR95_ID/far',\n",
    "    'AUROC_ID/near', 'FPR95_ID/near', 'AUROC_near/far',\n",
    "    'FPR95_near/far', 'AUROC_ID/near+far', 'FPR95_ID/near+far'\n",
    "]\n",
    "\n",
    "# Filtrar solo las columnas deseadas\n",
    "filtered_aux = aux[columns_to_consider]\n",
    "\n",
    "# Calcular la media de cada columna\n",
    "mean_values = filtered_aux.mean()\n",
    "\n",
    "# Calcular la desviación estándar de cada columna\n",
    "std_values = filtered_aux.std()\n",
    "\n",
    "# Combinar en un DataFrame con nombres de métricas\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': mean_values.index,\n",
    "    'Mean': mean_values.values,\n",
    "    'Standard Deviation': std_values.values\n",
    "})\n",
    "\n",
    "# Mostrar el resumen\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
